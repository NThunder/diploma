\section{Введение}
\label{sec:Chapter0} \index{Chapter0}

Оптическое распознавание текста (OCR) - это технология, которая позволяет компьютерам "читать" и интерпретировать текст, присутствующий на изображениях или сканах документов. Она находит применение в множестве областей, включая:

Автоматизация документооборота: Извлечение данных из счетов, накладных, контрактов.
Поиск по изображениям: Поиск изображений, содержащих определенный текст.
Помощь людям с ограниченными возможностями: Преобразование печатного текста в речь или шрифт Брайля.
Перевод текста на изображениях: Распознавание и перевод вывесок, меню и других надписей.

Задача: Совершенствование  OCR-систем для повышения точности распознавания текста, особенно  в сложных условиях (разнообразные шрифты, низкое качество изображения,  разные языки).

Актуальность:Несмотря на  значительный прогресс в области OCR,  существуют  факторы,  обуславливающие необходимость дальнейших исследований:

1. Ограниченное количество размеченных данных:Обучение  эффективных  OCR-моделей требует  больших объемов размеченных данных,  получение которых  дорого  и трудоемко.
2. Разнообразие  условий: OCR-системы должны быть устойчивыми к различным шрифтам,  ориентациям текста, качеству изображения и другим факторам,  усложняющим распознавание.
3. Сложность  некоторых языков: Распознавание  текста на  языках  с  иероглифической  письменностью,  таких как  японский,  представляет  особую  сложность.

Решение: Применение  semi-supervised  методов обучения,  таких как  предобучение с использованием  маскирования,  позволяет эффективно использовать  как размеченные,  так и неразмеченные данные,  что  особенно  важно для  OCR.  Это  открывает  новые  возможности для  повышения  точности  и  универсальности  OCR-систем. 

Определения понятий, которые понадобятся в постановке задачи:

1. Оптическое распознавание текста (OCR):Технология, позволяющая преобразовывать изображения печатного или рукописного текста в машиночитаемый формат. OCR-системы "читают" текст на изображениях и переводят его в символы, которые можно редактировать, искать и обрабатывать компьютером.

2. Semi-supervised обучение:Подход к машинному обучению, использующий как размеченные (с известными ответами), так и неразмеченные данные для обучения модели. Это особенно полезно, когда размеченных данных мало, а неразмеченных — много.

3. Предобучение с использованием маскирования (Masked Pre-training): Метод самообучения (self-supervised learning), при котором модель обучается восстанавливать замаскированные (скрытые) части входных данных.  В контексте OCR это может быть маскирование патчей изображения или элементов последовательности текста.

4. Патч изображения (Image Patch):Небольшой фрагмент изображения, обычно квадратной формы. В моделях, основанных на патчах, изображение разбивается на множество патчей, которые обрабатываются независимо или с учетом контекста.

5.  Входные представления декодера (Decoder Input Embeddings):В моделях энкодер-декодер, декодер принимает на вход  "сжатое" представление входных данных, созданное энкодером.  В контексте OCR,  входные представления декодера  могут содержать информацию о визуальных  признаках  изображения.

6.  Mask ratio: Параметр, определяющий процент входных данных, скрываемых (маскируемых) во время предобучения с использованием маскирования. 

7. CharAcc (Character Accuracy): Метрика оценки качества  OCR-систем, измеряющая процент правильно распознанных символов. 

8. Японский язык: Язык с  иероглифической  системой письма,  где  каждый символ может представлять целый слог  или  слово.   Распознавание японского текста представляет собой сложную задачу для OCR  из-за  большого  числа  символов и  их  сложной  структуры.

9. STR (Scene Text Recognition):  подвид OCR, который фокусируется на распознавании текста в естественных сценах, например, на вывесках, дорожных знаках, этикетках. Отличается от обычного OCR сложностью распознавания из-за разнообразных фонов, шрифтов, ракурсов и освещения.

10. Энкодер-декодер (Encoder-Decoder):  распространенная архитектура нейронных сетей, состоящая из двух частей:  энкодер преобразует входные данные в скрытое представление, а декодер  генерирует выходные данные на основе этого представления.  Широко используется в задачах перевода,  генерации текста и распознавания образов.

11. Baseline: базовая модель или результат, с которым сравниваются результаты  других моделей или  экспериментов. 

12. Предобучение (Pretraining):  этап обучения модели на большом наборе данных (обычно неразмеченных),  прежде чем она будет настроена под конкретную задачу.  Позволяет модели  выучить общие закономерности данных, что  повышает  эффективность  обучения на меньших, специализированных наборах данных.

13. Дообучение (Finetuning):  этап  настройки  предобученной  модели  под  конкретную задачу с использованием  меньшего, специализированного набора данных (обычно размеченных). На этом этапе  корректируются  веса модели,  чтобы  она  лучше  справлялась  с  заданной  задачей. 

14 .Vision Transformer (ViT): тип  нейронной  сети,  изначально  разработанный  для  обработки  изображений.  В  отличие  от  CNN, ViT  разбивает  изображение  на  патчи  и  обрабатывает  их  как  последовательности, используя  механизм  внимания.  

15. CTCLoss (Connectionist Temporal Classification Loss): функция  потерь,  используемая  для  обучения  моделей  распознавания  последовательностей  (например,  текста  или  речи),  когда  нет  чётких  границ  между  элементами  последовательности  во  входных  данных. 

16. Mask ratio:  параметр,  который  определяет,  какая  доля  входных  данных  будет  скрыта  при  маскировании.  Например,  mask ratio = 0.75  означает,  что  75 процентов  входных  данных  будут  замаскированы. 

17. MSE loss (Mean Squared Error Loss): функция потерь, которая вычисляет среднеквадратичную ошибку между предсказанными и целевыми значениями. Часто используется для задач регрессии.

18. Латентные признаки (Latent Features): Скрытые, не наблюдаемые напрямую характеристики данных, которые модель обучается извлекать.  В контексте STR,  латентные  признаки могут  отражать форму,  стиль,  семантику  символов или их сочетаний. 

19. Промежуточные латентные признаки (Intermediate Latent Features): Латентные признаки,  извлекаемые  моделью на промежуточных слоях энкодера или декодера, а не только на конечном слое.   

20. Архитектура модели (Model Architecture):Структура  нейронной  сети,  описывающая,  как  организованы  и  связаны  между  собой  её  слои  и  блоки.  Выбор  архитектуры  влияет на  способности  модели  извлекать  признаки  и  решать  задачу.

\newpage
