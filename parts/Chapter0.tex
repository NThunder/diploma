\section{Введение}
\label{sec:Chapter0} \index{Chapter0}
\subsection{Задача распознавания текста}
Распознавание текста (Optical Character Recognition, OCR) -- это задача в области искусственного интеллекта, целью которой является автоматизация процесса преобразования изображений, содержащих текст, в машиночитаемый текстовый формат. Данная задача  имеет  широкий  спектр  применений,  включая  автоматизацию  документооборота,  поиск  по  изображениям,  системы  помощи  водителям,  и  многие  другие.
\begin{itemize}

    \item Типы задач распознавания текста

    Существует несколько основных категорий типа входных данных по домену,  характеризующихся содержанием изображения и типом текста на нем:

    \begin{itemize}
        \item \textbf{OCR (Optical Character Recognition):}  Классический  OCR  предназначен  для  распознавания  печатного  или  машинописного  текста как в  относительно  простых  условиях,  например,  в  отсканированных  документах  с  хорошим  качеством  изображения  и  четким  отделением  текста  от  фона,  так и в более сложных сценариях, включающих фотографии низкого разрешения с мелким текстом. 
        \item \textbf{STR (Scene Text Recognition):}  STR  специализируется  на  распознавании  текста  в  естественных  сценах,  где  текст  может  быть  изображен  на  сложных  фонах,  с  различными  шрифтами,  цветами,  перспективой  и  освещением. 
        \item \textbf{HWR (Handwritten Text Recognition):}  HWR  предназначен  для  распознавания  рукописного  текста,  что  является  особенно  сложной  задачей  из-за  вариативности  почерков,  стилей  письма  и  возможных  дефектов  изображения. 
    \end{itemize}

    \item{Основные этапы распознавания текста}

    Несмотря  на  разнообразие  подходов,  большинство  систем  распознавания  текста  реализуют  следующие  этапы:

    \begin{enumerate}
        \item \textbf{Предобработка изображения:}  Этот  этап  включает  в  себя  ряд  операций,  направленных  на  улучшение  качества  изображения  и  выделение  текстовых  областей:  бинаризацию и  шумоподавление.
        \item \textbf{Извлечение признаков:}  На  этом  этапе  из  изображений  символов  или  слов  извлекаются  дискриминативные  признаки,  характеризующие  их  визуальные  свойства.  Для  этого  могут  использоваться  различные  методы  обработки  изображений,  в  том  числе  и  глубокое  обучение. 
        \item \textbf{Классификация:}  Извлеченные  признаки  подаются  на  вход  классификатора  (например,  нейронной  сети),  который  определяет,  какой  символ  или  слово  представлены  на  изображении. 
        \item \textbf{Постобработка:}  На  этом  этапе  могут  применяться  дополнительные  методы  для  улучшения  точности  распознавания,  такие  как  языковые  модели,  коррекция  ошибок  и  др. 
    \end{enumerate}

    \item{Сложности и вызовы}

    Разработка  эффективных  и  универсальных  систем  распознавания  текста  сталкивается  с  рядом  сложностей:

    \begin{itemize}
        \item \textbf{Разнообразие  условий  съемки:}  Освещение,  ракурс,  качество  изображения  и  другие  факторы  могут  значительно  влиять  на  качество  изображения  и  затруднять  распознавание  текста. 
        \item \textbf{Разнообразие  шрифтов  и  языков:}  Существует  огромное  количество  шрифтов,  стилей  письма  и  языков,  что  затрудняет  создание  универсальных  систем,  способных  распознавать  любой  текст, например распознавание  текста на  языках  с  иероглифической  письменностью,  таких как  японский,  представляет  особую  сложность. 
        \item \textbf{Сложные  фоны:}  Распознавание  текста  на  сложных  фонах  (например,  с  множеством  объектов,  теней,  градиентов)  представляет  собой  сложную  задачу.
        \item \textbf{Ограниченное количество размеченных данных:}  Обучение  эффективных  OCR-моделей требует  больших объемов размеченных данных,  получение которых  дорого  и трудоемко. 
    \end{itemize}
\end{itemize}
\subsection{Semi-supervised предобучение моделей}
Semi-supervised предобучение моделей — это подход в машинном обучении, который стремится повысить эффективность моделей, используя как размеченные, так и неразмеченные данные одновременно в процессе предобучения.  Этот подход особенно актуален в условиях, когда объём неразмеченных данных значительно превышает объём размеченных данных.
\begin{itemize}
    \item Суть метода
    
    В отличие от self-supervised обучения, где этапы работы с размеченными и неразмеченными данными разделены, semi-supervised предобучение предполагает их совместное использование. Модель обучается  на  комбинированном наборе данных,  стремясь:

    \begin{enumerate}
        \item \textbf{Извлечь знания из размеченных данных:} Модель обучается стандартным способом на размеченных данных, получая информацию о связях между входными данными и целевыми переменными.
        \item \textbf{Использовать структуру неразмеченных данных:} Параллельно модель пытается выявить скрытые закономерности и структуру в неразмеченных данных.  Это может быть достигнуто, например, путём предсказания пропущенных значений.
    \end{enumerate}

    \item{Преимущества semi-supervised предобучения}

    \begin{itemize}
        \item \textbf{Повышение эффективности использования данных:} Semi-supervised предобучение позволяет извлекать информацию не только из  размеченных,  но  и  из  неразмеченных  данных,  что  особенно  важно  при  ограниченном  объеме  размеченных  данных.
        \item \textbf{Улучшение обобщающей способности:} Использование  информации,  содержащейся  в  неразмеченных  данных,  может  помочь  модели  лучше  обобщать  знания  и  показывать  более  высокую  производительность  на  незнакомых  данных.
        \item \textbf{Сокращение потребности в размеченных данных:} Semi-supervised обучение  позволяет  снизить  потребность  в  дорогостоящей  и  трудоемкой  разметке  больших  объемов  данных.
    \end{itemize}

\end{itemize}
\subsection{Стратегии маскирования в нейронных сетях}
Маскирование (masking) -- это важный метод, используемый в нейронных сетях для выборочной обработки или игнорирования информации. Он широко применяется как на этапе предобучения, так и во время обучения модели для решения конкретных задач. 

Существует несколько уровней, на которых можно применять маскирование в нейронных сетях:
\begin{itemize}
    \item{Маскирование на уровне входных данных}

    \begin{itemize}
        \item \textbf{Маскирование патчей изображения (Image Patch Masking):} Используется преимущественно в моделях компьютерного зрения. Случайные патчи (участки) изображения заменяются заглушками (например, нулями, средним значением пикселей или случайным шумом). 
        
        Пример: Masked Autoencoders \cite{he2021masked}, SimMIM \cite{xie2021simmim}.
        \item \textbf{Маскирование токенов (Token Masking):} Применяется в моделях обработки естественного языка (NLP). Случайные токены (слова или подслова) в последовательности заменяются специальным токеном MASK. 
        
        Пример: BERT \cite{devlin2018bert}.
        \item \textbf{Маскирование последовательностей (Sequence Masking):} Используется для работы с последовательностями разной длины.  Элементы последовательности, следующие за определенной точкой, маскируются. 
        
        Пример: RNN для обработки текста с переменной длиной.
    \end{itemize}

    \item{Маскирование на уровне скрытых представлений (Latent Representations)}

    \begin{itemize}
        \item \textbf{Маскирование признаков (Feature Masking):} Внутренние признаки (features), получаемые на промежуточных слоях нейронной сети, выборочно маскируются.
        
        Пример: Dropout (можно рассматривать как форму маскирования признаков).
        \item \textbf{Маскирование внимания (Attention Masking):} В архитектурах с механизмом внимания (attention mechanism), маска используется для предотвращения внимания к определенным частям входной последовательности. 
        
        Пример: Transformer для машинного перевода, где маска предотвращает внимание к словам, расположенным правее текущего обрабатываемого слова.
    \end{itemize}


    \item{Преимущества маскирования}

    \begin{itemize}
        \item \textbf{Регуляризация:} Предотвращает переобучение, вынуждая модель обучаться более общим и устойчивым представлениям.
        \item \textbf{Самообучение (Self-Supervised Learning):} Маскирование создает искусственную задачу восстановления скрытой информации, что позволяет модели обучаться на неразмеченных данных.
        \item \textbf{Обработка вариативности данных:} Позволяет эффективно работать с данными разной длины, пропущенными значениями и другими видами вариативности.
    \end{itemize}
\end{itemize}
\subsection{Стратегии маскирования изображений}
Маскирование изображений (MIM) – это подход к самообучению моделей компьютерного зрения, основанный на принципе маскирования части изображения и обучении модели на его восстановление. 
\begin{itemize}
    \item{Общая схема MIM}

    \begin{enumerate}
        \item \textbf{Маскирование:} Часть изображения скрывается маской.
        \item \textbf{Кодирование:} Замаскированное изображение подаётся на вход энкодера, который формирует скрытое представление
        
         (latent representation).
        \item \textbf{Декодирование:} Декодер получает скрытое представление и пытается восстановить исходное изображение (или его замаскированные части).
        \item \textbf{Обучение:} Модель обучается минимизировать разницу между восстановленным и оригинальным изображением.
    \end{enumerate}

    \item{Различные стратегии маскирования в MIM}

    \begin{itemize}
        \item Размеры и форма маски:
            \begin{itemize}
                \item Блочная маска (Block Masking): Изображение разбивается на блоки, и некоторые блоки скрываются.
                
                Пример: BEiT \cite{bao2021beit}.
                \item Случайная маска (Random Masking): Пиксели маскируются случайным образом с определенной вероятностью.
                 
                Пример:MAE \cite{he2021masked}.
                \item Структурированная маска (Structured Masking): Маска имеет определенную структуру, например, линии, круги, сетки. 

                Пример:CutOut \cite{devries2017improved}.
            \end{itemize}
        \item Тип маски:
            \begin{itemize}
                \item Бинарная маска (Binary Masking): Пиксели либо скрыты, либо нет (1 или 0).
                \item Непрерывная маска (Continuous Masking): Интенсивность пикселей маскируется частично, например, умножается на значение от 0 до 1.
            \end{itemize}
        \item Стратегии предсказания:
            \begin{itemize}
                \item Восстановление пикселей (Pixel Reconstruction): Модель предсказывает значения пикселей замаскированных областей.
                \item Предсказание признаков (Feature Reconstruction): Модель предсказывает признаки (features) скрытых областей на некотором уровне энкодера. 
                \item Предсказание токенов (Token Prediction): Скрытое представление квантуется в дискретные токены, и модель предсказывает эти токены.
            \end{itemize}
    \end{itemize}

    \item{Преимущества MIM}
    \begin{itemize}
        \item Эффективное самообучение: Позволяет обучать модели на огромных объемах неразмеченных изображений.
        \item Улучшение качества представлений: Модели, обученные с использованием MIM, формируют более информативные и устойчивые представления изображений.
        \item Широкая применимость: Предварительно обученные модели MIM могут использоваться для решения различных задач компьютерного зрения: классификации, обнаружения объектов, сегментации и др.
    \end{itemize}
\end{itemize}
\subsection{Терминологический глоссарий}

\begin{enumerate}

    \item \textbf{Патч изображения (Image Patch):} Небольшой фрагмент изображения, обычно квадратной формы, на которые разбивается изображение для обработки в моделях компьютерного зрения, таких как Vision Transformers \cite{dosovitskiy2020image}.

    \item \textbf{Признаки (Features):}  Измеримые  характеристики  данных,  которые  модель  использует  для  своего  обучения  и  принятия  решений.  В  контексте  OCR,  признаки  могут  включать  в  себя  форму,  размер,  текстуру  и  расположение  символов.

    \item \textbf{Внимания (Attention):} Механизм,  используемый  в  нейронных  сетях,  позволяющий  модели  сосредоточиться  на  наиболее  важных  частях  входных  данных  при  выполнении  задачи.

    \item \textbf{Mask ratio:}  Параметр  в  методах  маскирования,  определяющий  долю  входных  данных,  которая  будет  скрыта  от  модели  во  время  обучения.  

    \item \textbf{Архитектура модели (Model Architecture):} Описание структуры и организации нейронной сети, включая типы используемых слоев, их количество, связи между ними и другие параметры.

    \item \textbf{Точность распознавания (Accuracy):}  Метрика,  используемая  для  оценки  качества  работы  OCR-систем.  Измеряется  как  процент  правильно  распознанных  символов  или  слов  в  тексте.

    \item \textbf{Японский язык:} Язык с  иероглифической  системой письма,  где  каждый символ может представлять целый слог  или  слово.   Распознавание японского текста представляет собой сложную задачу для OCR  из-за  большого  числа  символов и  их  сложной  структуры.

    \item \textbf{Размер патча:} Параметр, определяющий размер патчей изображения, на которые оно разбивается для обработки в моделях компьютерного зрения.

    \item \textbf{Нейронная сеть (Neural Network):} Вычислительная модель, вдохновленная структурой и функциями мозга, состоящая из взаимосвязанных узлов (нейронов), организованных в слои. Каждый нейрон выполняет простую математическую операцию над своим входом, а связи между нейронами имеют веса, которые корректируются в процессе обучения для выполнения целевой задачи.

    \item \textbf{Сверточная нейронная сеть (CNN):} Специализированный тип нейронной сети, предназначенный для эффективной обработки изображений. CNN используют операцию свертки для извлечения локальных признаков на разных уровнях абстракции, что позволяет им эффективно распознавать образы, объекты и текстуры.
    
    \item \textbf{Transformer:} Архитектура нейронной сети, основанная на механизме внимания (attention), что позволяет ей эффективно обрабатывать последовательности данных, таких как текст или временные ряды. Transformer не использует рекуррентные связи, как RNN, а обрабатывает все элементы последовательности параллельно, что значительно ускоряет обучение и позволяет модели улавливать зависимости на больших расстояниях. 

    \item \textbf{Рекуррентная нейронная сеть (RNN, Recurrent Neural Network):}  Тип  нейронной  сети,  специализированный  для  обработки  последовательных  данных,  таких  как  текст  или  временные  ряды.  RNN  обладают  памятью,  позволяющей  им  учитывать  предыдущие  элементы  последовательности  при  обработке  текущего  элемента.

    \item \textbf{Энкодер-декодер (Encoder-Decoder):}  распространенная архитектура нейронных сетей, состоящая из двух частей:  энкодер преобразует входные данные в скрытое представление, а декодер  генерирует выходные данные на основе этого представления.  Широко используется в задачах перевода,  генерации текста и распознавания образов.

    \item  \textbf{Входные представления декодера (Decoder Input Embeddings):} В моделях энкодер-декодер, декодер принимает на вход  "сжатое"
    
    представление входных данных, созданное энкодером.  В контексте OCR,  входные представления декодера  могут содержать информацию о визуальных  признаках  изображения.

    \item  \textbf{Baseline:} базовая модель или результат, с которым сравниваются результаты  других моделей или  экспериментов. 

    \item  \textbf{Предобучение (Pretraining):}  этап обучения модели на большом наборе данных (обычно неразмеченных),  прежде чем она будет настроена под конкретную задачу.  Позволяет модели  выучить общие закономерности данных, что  повышает  эффективность  обучения на меньших, специализированных наборах данных.

    \item  \textbf{Дообучение (Finetuning):}  этап  настройки  предобученной  модели  под  конкретную задачу с использованием  меньшего, специализированного набора данных (обычно размеченных). На этом этапе  корректируются  веса модели,  чтобы  она  лучше  справлялась  с  заданной  задачей. 

    \item  \textbf{ Vision Transformer (ViT) \cite{dosovitskiy2020image}:} тип  нейронной  сети,  изначально  разработанный  для  обработки  изображений.  В  отличие  от  CNN, ViT  разбивает  изображение  на  патчи  и  обрабатывает  их  как  последовательности, используя  механизм  внимания.  

    \item  \textbf{ CTCLoss (Connectionist Temporal Classification Loss): \cite{graves2006connectionist}} функция  потерь,  используемая  для  обучения  моделей  распознавания  последовательностей  (например,  текста  или  речи),  когда  нет  чётких  границ  между  элементами  последовательности  во  входных  данных. 

    \item  \textbf{ ММSE loss (Mask Mean Squared Error Loss):} функция потерь, которая вычисляет среднеквадратичную ошибку  между исходными значениями пикселей в замаскированных областях и значениями пикселей, предсказанными моделью.

    \item \textbf{Функция потерь перекрестной энтропии (CrossEntropyLoss):} Функция потерь, часто используемая в задачах классификации, которая измеряет разницу между двумя распределениями вероятностей: предсказанным распределением вероятностей по классам и истинным распределением. Она штрафует модель за неверные предсказания и побуждает ее выдавать вероятности, более близкие к истинным меткам.

    \item  \textbf{Латентные признаки (Latent Features):} Скрытые, не наблюдаемые напрямую характеристики данных, которые модель обучается извлекать.  В контексте STR,  латентные  признаки могут  отражать форму,  стиль,  семантику  символов или их сочетаний. 

    \item  \textbf{Промежуточные латентные признаки:} Латентные признаки,  извлекаемые  моделью на промежуточных слоях энкодера или декодера, а не только на конечном слое.   

    \item  \textbf{Edit Distance(расстояние Левенштейна):}  метрика, используемая для измерения  различия между двумя последовательностями.  Определяется как минимальное количество операций редактирования (вставка, удаление, замена),  необходимых для преобразования одной последовательности в другую.

    \item  \textbf{Character Error Rate (CER):}  метрика, которая представляет  собой  нормированное  расстояние  Левенштейна между  распознанным  и  эталонным  текстом. 

    \item  \textbf{Character Accuracy (CharAcc):}  100 - CER.
    
    \item  \textbf{Word Accuracy (WordAcc):}  метрика, которая представляет долю  слов,  для  которых  расстояние  Левенштейна  между  распознанным  и  эталонным  словом  равно  нулю  (т.е.  слова  совпадают  полностью).

    \item  \textbf{Fragment Accuracy (FragAcc):}  точность распознавания на уровне фрагментов текста.

    \item  \textbf{Hidden size:} количество признаков (измерений) в векторах скрытого состояния, которые обрабатываются на каждом слое энкодера и декодера. 

    \item  \textbf{Слой (layer):} блок обработки информации, состоящий из механизмов внимания и полносвязных сетей, которые применяются последовательно для анализа связей между элементами последовательности и формирования их контекстуального представления. 

    \item  \textbf{Размерность feedforward:} размер скрытого слоя в полносвязной сети, которая применяется к каждому токену внутри слоя энкодера или декодера для нелинейного преобразования признаков. Обычно размерность feedforward в несколько раз больше, чем hidden size модели. 

    \item  \textbf{Эпоха обучения:}  этап  обучения модели, который  характеризуется определенным количеством  шагов оптимизации (итераций). В конце каждой эпохи, как правило, проводится оценка качества модели на валидационной выборке и обновление параметров оптимизации,  например,  снижение  learning rate.

    \item  \textbf{ Батч данных (batch):} порция данных из всего набора, которая подается на вход модели машинного обучения за один раз для вычисления градиентов и обновления весов модели во время обучения. 

    \item  \textbf{Обучение модели:} итеративный процесс настройки параметров (весов) модели машинного обучения на основе обучающих данных с целью минимизации ошибки на данных, которые модель не видела ранее.
    \item  \textbf{Контекстуальные связи:} зависимости и взаимосвязи между элементами данных, при которых значение и интерпретация одного элемента определяются окружающими его элементами.  
    \item  \textbf{Masked Language Modeling, MLM:} метод обучения языковых моделей, при котором случайные токены во входном тексте маскируются (заменяются специальным токеном, например, [MASK]), а модель должна предсказать эти замаскированные токены, основываясь на контексте оставшихся токенов.
    \item  \textbf{Переобучение (overfitting):} явление в машинном обучении, когда модель слишком хорошо запоминает обучающие данные, включая шум и случайные отклонения. В результате модель показывает отличные результаты на обучающих данных, но плохо работает на новых данных, отличных от тех, что видела модель при настройке.
    \item  \textbf{Латентное представление (latent representation):} сжатое по размерности, закодированное представление данных, которое захватывает наиболее важные характеристики исходной информации, часто в форме, недоступной для прямого человеческого понимания. 
    \item  \textbf{Dropout:} метод регуляризации, используемый в нейронных сетях для предотвращения переобучения. Он заключается в случайном "выключении" (игнорировании) определенной доли нейронов во время каждого шага обучения.
    \item  \textbf{Компьютерное зрение (Computer Vision, CV):} междисциплинарная область, занимающаяся разработкой теории и методов,  позволяющих компьютерам "видеть"   и интерпретировать визуальную информацию из окружающего мира, получаемую с помощью камер и других сенсоров.
    \item  \textbf{Контролируемое обучение (supervised learning):} парадигма машинного обучения, в которой алгоритм строит модель, отображающую входные данные в выходные,  оптимизируя  её  параметры  на  основе  набора  данных  с  известными  парами  "вход - желаемый  выход". 
    \item  \textbf{Транскрипция исторических документов:} задача автоматического преобразования рукописных или машинописных исторических документов в машиночитаемый текстовый формат с использованием методов компьютерного зрения и обработки естественного языка. 
\end{enumerate} 
    

\newpage
