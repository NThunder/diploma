\section{Обзор существующих решений}
\label{sec:Chapter2} \index{Chapter2}


В  области  semi-supervised  предобучения  OCR и CV моделей  с  использованием  маскирования  существует  ряд  исследований, предлагающих  различные  стратегии  и  архитектуры.
\subsection{Маскирование  патчей  изображения}
Метод заключается в маскировании случайных патчей входного изображения.  Модель должна восстановить скрытые патчи на основе видимой информации. Этот подход демонстрирует  хорошие  результаты  в  задачах  компьютерного  зрения,  однако  его  эффективность  для  OCR  может  быть  ниже  из-за  того,  что  текст  имеет  более  структурированный  характер,  чем  общие  изображения.
\begin{itemize}
    \item \textbf{MAE: \cite{he2021masked}}  Модель  обучается  восстанавливать  случайно  замаскированные  патчи  изображения,  что  позволяет  ей  изучать  глобальные  зависимости  между  различными  частями  изображения.

    \item \textbf{SimMIM: \cite{xie2021simmim}}  Предлагает  упрощенный  подход  к  маскированию  изображений,  используя  простую  линейную  декодирующую  голову  для  восстановления  скрытых  патчей.

    \item \textbf{SupMAE: \cite{gao2022supmae}} В отличие от MAE, использующего только неразмеченные данные, SupMAE предлагает использовать контролируемое обучение на размеченных данных. В процессе обучения SupMAE предсказывает не пиксели изображения, а целевые метки для каждого замаскированного патча.  Эксперименты показывают, что SupMAE  эффективнее стандартного MAE, особенно при ограниченном количестве обучающих данных.
 
    \item \textbf{Revisiting Scene Text Recognition: A Data Perspective: \cite{baek2019revisiting}}  В  этой  работе  исследуется  влияние  объема  и  разнообразия  данных  на  эффективность  моделей  распознавания  текста.  Авторы  демонстрируют,  что  большие  и  разнообразные  наборы  данных  критически  важны  для  достижения  высокой  точности  распознавания.  
\end{itemize}

\subsection{Маскирование входных представлений декодера}
Этот  подход  заключается  в  маскировании  части  входных  представлений,  которые  декодер  получает  от  энкодера.  Декодер  должен  научиться  восстанавливать  замаскированные  части  на  основе  контекста.
\begin{itemize}
    \item  \textbf{Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource Historical Document Transcription: \cite{stammer2023lacuna}} Представлен  метод  самообучения,  специально  разработанный  для  транскрипции  исторических  документов,  где  объём  размеченных  данных  ограничен.  Модель  обучается  восстанавливать  пропущенные  фрагменты  текста  (лакуны),  что  позволяет  ей  адаптироваться  к  особенностям  старинных  шрифтов  и  стилей  письма.
    Метод  может  быть  эффективен  для  OCR,  так  как  он  позволяет  модели  лучше  учитывать  контекст  при  распознавании  символов.
\end{itemize}

\subsection{Маскирование  патчей  изображения  и  элементов  последовательности  текста}  Некоторые  работы  исследуют  комбинацию  маскирования  на  разных  уровнях,  чтобы  использовать  как  визуальную,  так  и  контекстуальную  информацию  для  более  точного  распознавания  текста. 
\begin{itemize}
    \item \textbf{Masked Vision-Language Transformers for Scene Text Recognition: \cite{lyons2022masked}}  В  этой  работе представлена  модель,  которая  совместно  обучается  на  замаскированных  изображениях  и  текстовых  описаниях,  что  позволяет  ей  эффективно  извлекать  как  визуальные,  так  и  семантические  признаки  для  распознавания  текста.
    \item \textbf{MaskOCR: \cite{li2022maskocr}}  Предложен  метод  предобучения  OCR-моделей  с  использованием  маскирования  как  на  уровне  изображения,  так  и  на  уровне  текста.  Модель  с  архитектурой  энкодер-декодер  учится  восстанавливать  замаскированные  патчи  изображения  и  генерировать  соответствующую  текстовую  последовательность.  Эксперименты  показали,  что  такой  подход  позволяет  эффективно  использовать  неразмеченные  данные  и  улучшает  обобщающую  способность  модели.
\end{itemize}

\subsection{Ограничения  существующих  подходов}

\begin{itemize}
    \item \textbf{Большинство  работ  сосредоточено  на  английском  языке:}  Исследования  по  маскированию  для  OCR  на  других  языках,  особенно  с  иероглифической  системой  письма,  остаются  ограниченными.
    \item \textbf{Не иследовано комбинирование подходов:}  Комбинирование  маскирования  патчей  изображения  и  входных  представлений  декодера может потенциально  привести  к  наилучшим  результатам,  так  как  позволяет  модели  учитывать  как  локальную  информацию  (из  патчей  изображения),  так  и  глобальный  контекст  (из  входных  представлений  декодера).
\end{itemize}

\newpage
